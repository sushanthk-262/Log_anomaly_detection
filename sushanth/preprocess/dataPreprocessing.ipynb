{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#Parsing For a Demo HDFS dataset# "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UxQLk-aeNZ1P",
        "outputId": "68c295cd-f75a-47f9-f357-cc695e2ed364"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Parsing file: E:\\Log_anomaly_detection\\Log_anomaly_detection\\sushanth\\preprocess\\logs\\HDFS_2k.log\n",
            "Total lines:  2000\n",
            "Processed 50.0% of log lines.\n",
            "Processed 100.0% of log lines.\n",
            "Parsing done. [Time taken: 0:00:01.446507]\n",
            "      LineId   Date    Time    Pid Level                     Component  \\\n",
            "0          1  81109  203615    148  INFO  dfs.DataNode$PacketResponder   \n",
            "1          2  81109  203807    222  INFO  dfs.DataNode$PacketResponder   \n",
            "2          3  81109  204005     35  INFO              dfs.FSNamesystem   \n",
            "3          4  81109  204015    308  INFO  dfs.DataNode$PacketResponder   \n",
            "4          5  81109  204106    329  INFO  dfs.DataNode$PacketResponder   \n",
            "...      ...    ...     ...    ...   ...                           ...   \n",
            "1995    1996  81111  101621  24902  INFO      dfs.DataNode$DataXceiver   \n",
            "1996    1997  81111  101735  26595  INFO  dfs.DataNode$PacketResponder   \n",
            "1997    1998  81111  101804  26494  INFO      dfs.DataNode$DataXceiver   \n",
            "1998    1999  81111  101954  26414  INFO  dfs.DataNode$PacketResponder   \n",
            "1999    2000  81111  102017  26347  INFO      dfs.DataNode$DataXceiver   \n",
            "\n",
            "                                                Content   EventId  \\\n",
            "0     PacketResponder 1 for block blk_38865049064139...  dc2c74b7   \n",
            "1     PacketResponder 0 for block blk_-6952295868487...  dc2c74b7   \n",
            "2     BLOCK* NameSystem.addStoredBlock: blockMap upd...  5d5de21c   \n",
            "3     PacketResponder 2 for block blk_82291938032499...  dc2c74b7   \n",
            "4     PacketResponder 2 for block blk_-6670958622368...  dc2c74b7   \n",
            "...                                                 ...       ...   \n",
            "1995  Receiving block blk_4198733391373026104 src: /...  09a53393   \n",
            "1996  Received block blk_-5815145248455404269 of siz...  e3df2680   \n",
            "1997  Receiving block blk_-295306975763175640 src: /...  09a53393   \n",
            "1998  PacketResponder 0 for block blk_52257196770490...  dc2c74b7   \n",
            "1999  Receiving block blk_4343207286455274569 src: /...  09a53393   \n",
            "\n",
            "                                          EventTemplate  \\\n",
            "0         PacketResponder <*> for block <*> terminating   \n",
            "1         PacketResponder <*> for block <*> terminating   \n",
            "2     BLOCK* NameSystem.addStoredBlock: blockMap upd...   \n",
            "3         PacketResponder <*> for block <*> terminating   \n",
            "4         PacketResponder <*> for block <*> terminating   \n",
            "...                                                 ...   \n",
            "1995             Receiving block <*> src: <*> dest: <*>   \n",
            "1996            Received block <*> of size <*> from <*>   \n",
            "1997             Receiving block <*> src: <*> dest: <*>   \n",
            "1998      PacketResponder <*> for block <*> terminating   \n",
            "1999             Receiving block <*> src: <*> dest: <*>   \n",
            "\n",
            "                                          ParameterList  \n",
            "0                        ['1', 'blk_38865049064139660']  \n",
            "1                     ['0', 'blk_-6952295868487656571']  \n",
            "2     ['10.251.73.220:50010', 'blk_71283702376877284...  \n",
            "3                      ['2', 'blk_8229193803249955061']  \n",
            "4                     ['2', 'blk_-6670958622368987959']  \n",
            "...                                                 ...  \n",
            "1995  ['blk_4198733391373026104', '/10.251.106.10:46...  \n",
            "1996  ['blk_-5815145248455404269', '67108864', '/10....  \n",
            "1997  ['blk_-295306975763175640', '/10.250.9.207:532...  \n",
            "1998                   ['0', 'blk_5225719677049010638']  \n",
            "1999  ['blk_4343207286455274569', '/10.250.9.207:597...  \n",
            "\n",
            "[2000 rows x 10 columns]\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import re\n",
        "\n",
        "import sys\n",
        "from logparser import Drain\n",
        "\n",
        "input_log_file = 'E:\\Log_anomaly_detection\\Log_anomaly_detection\\sushanth\\preprocess\\logs\\HDFS_2k.log'\n",
        "\n",
        "\n",
        "# data_list = []\n",
        "\n",
        "# with open(input_file, 'r') as file:\n",
        "#     for line in file:\n",
        "#         columns = line.split()\n",
        "\n",
        "#         if len(columns) >= 6:\n",
        "#             data_list.append(columns[5:])\n",
        "\n",
        "# file.close()\n",
        "\n",
        "# for item in data_list:\n",
        "#     print(item)\n",
        "\n",
        "\n",
        "input_dir = \"\\logs\"  # The input directory of log file\n",
        "output_dir = \"\\parse\"  # The output directory of parsing results\n",
        "log_file_all = input_log_file # The input log file name\n",
        "log_format = \"<Date> <Time> <Pid> <Level> <Component>: <Content>\"  # HDFS log format\n",
        "# Regular expression list for optional preprocessing (default: [])\n",
        "regex = [\n",
        "    r\"blk_(|-)[0-9]+\",  # block id\n",
        "    r\"(/|)([0-9]+\\.){3}[0-9]+(:[0-9]+|)(:|)\",  # IP\n",
        "    r\"(?<=[^A-Za-z0-9])(\\-?\\+?\\d+)(?=[^A-Za-z0-9])|[0-9]+$\",  # Numbers\n",
        "]\n",
        "st = 0.5  # Similarity threshold\n",
        "depth = 4  # Depth of all leaf nodes\n",
        "\n",
        "# run on training dataset\n",
        "parser = Drain.LogParser(\n",
        "    log_format, indir=input_dir, outdir=output_dir, depth=depth, st=st, rex=regex\n",
        ")\n",
        "parser.parse(log_file_all)\n",
        "\n",
        "structuredLog = open(\"HDFS_2k.log_structured.csv\",\"r\")\n",
        "df = pd.read_csv(structuredLog)\n",
        "\n",
        "print(df)\n",
        "\n",
        "# selected_columns = df[['EventId', 'ParameterList']]\n",
        "\n",
        "# # Combine 'EventId' and 'ParameterList' into a new column 'CombinedParameterList'\n",
        "# selected_columns['CombinedParameterList'] = selected_columns.apply(lambda row: f\"{row['EventId']}, {row['ParameterList']}\", axis=1)\n",
        "\n",
        "# # Save the result to a new CSV file\n",
        "# selected_columns.to_csv('features.csv', index=False)\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#Grouping for demo HDFS datset#"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "#grouping for demo files\n",
        "import csv\n",
        "\n",
        "input_log_file = 'E:\\Log_anomaly_detection\\Log_anomaly_detection\\sushanth\\preprocess\\parse\\demo_data\\HDFS_2k.log_structured.csv'\n",
        "\n",
        "def extract_data(input_file, output_file):\n",
        "    with open(input_file, 'r') as csvfile:\n",
        "        reader = csv.DictReader(csvfile)\n",
        "        \n",
        "        # Extract EventId and strings of the form \"blk_*\"\n",
        "        data_list = [(row['EventId'], param) for row in reader for param in row['ParameterList'].strip(\"[]\").replace(\"'\", \"\").split(', ') if param.startswith('blk_')]\n",
        "\n",
        "    with open(output_file, 'w', newline='') as csvfile:\n",
        "        fieldnames = ['EventId', 'BlockString']\n",
        "        writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n",
        "        \n",
        "        writer.writeheader()\n",
        "        for data in data_list:\n",
        "            writer.writerow({'EventId': data[0], 'BlockString': data[1]})\n",
        "\n",
        "extract_data(input_log_file, 'E:\\Log_anomaly_detection\\Log_anomaly_detection\\sushanth\\preprocess\\group\\ blk_eve_ids.csv')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [],
      "source": [
        "#grouping part 2 for demo files\n",
        "\n",
        "import csv\n",
        "from collections import defaultdict\n",
        "\n",
        "def extract_and_group_data(input_file, output_file):\n",
        "    block_events = defaultdict(list)\n",
        "\n",
        "    with open(input_file, 'r') as csvfile:\n",
        "        reader = csv.DictReader(csvfile)\n",
        "        \n",
        "        # Extract EventId and strings of the form \"blk_*\"\n",
        "        for row in reader:\n",
        "            event_id = row['EventId']\n",
        "            for param in row['BlockString'].strip(\"[]\").replace(\"'\", \"\").split(', '):\n",
        "                if param.startswith('blk_'):\n",
        "                    block_id = param\n",
        "                    block_events[block_id].append(event_id)\n",
        "\n",
        "    with open(output_file, 'w', newline='') as csvfile:\n",
        "        fieldnames = ['BlockId', 'EventIds']\n",
        "        writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n",
        "        \n",
        "        writer.writeheader()\n",
        "        for block_id, event_ids in block_events.items():\n",
        "            writer.writerow({'BlockId': block_id, 'EventIds': ', '.join(event_ids)})\n",
        "\n",
        "extract_and_group_data('E:\\Log_anomaly_detection\\Log_anomaly_detection\\sushanth\\preprocess\\group\\demo\\ blk_eve_ids.csv', 'E:\\Log_anomaly_detection\\Log_anomaly_detection\\sushanth\\preprocess\\group\\demo\\grouped_output_file.csv')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#Grouping for actual project data# "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Extraction complete. Check the output file: E:\\Log_anomaly_detection\\Log_anomaly_detection\\sushanth\\preprocess\\group\\proj\\ blk_eve.csv\n"
          ]
        }
      ],
      "source": [
        "#grouping for proj data HDFS!\n",
        "\n",
        "import csv\n",
        "import re\n",
        "\n",
        "def extract_event_and_block(content):\n",
        "    # Define a regular expression to extract \"blk_*\" pattern\n",
        "    block_pattern = r'blk_[\\d]+'\n",
        "    \n",
        "    # Find all occurrences of \"blk_*\" in the content\n",
        "    blocks = re.findall(block_pattern, content)\n",
        "    \n",
        "    return blocks\n",
        "\n",
        "# Input and output file paths\n",
        "input_file = 'E:\\Log_anomaly_detection\\Log_anomaly_detection\\sushanth\\preprocess\\parse\\proj_data\\HDFS_100k.log_structured.csv' \n",
        "output_file = 'E:\\Log_anomaly_detection\\Log_anomaly_detection\\sushanth\\preprocess\\group\\proj\\ blk_eve.csv'\n",
        "\n",
        "with open(input_file, 'r') as infile, open(output_file, 'w', newline='') as outfile:\n",
        "    reader = csv.DictReader(infile)\n",
        "    fieldnames = ['EventId', 'Block']\n",
        "    writer = csv.DictWriter(outfile, fieldnames=fieldnames)\n",
        "    \n",
        "    writer.writeheader()\n",
        "    \n",
        "    for row in reader:\n",
        "        event_id = row['EventId']\n",
        "        content = row['Content']\n",
        "        blocks = extract_event_and_block(content)\n",
        "        \n",
        "        for block in blocks:\n",
        "            writer.writerow({'EventId': event_id, 'Block': block})\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Grouping complete. Check the output file: E:\\Log_anomaly_detection\\Log_anomaly_detection\\sushanth\\preprocess\\group\\proj\\sequence.csv\n"
          ]
        }
      ],
      "source": [
        "#grouping part 2 fro proj data HDFS\n",
        "import pandas as pd\n",
        "\n",
        "input_file = 'E:\\Log_anomaly_detection\\Log_anomaly_detection\\sushanth\\preprocess\\group\\proj\\ blk_eve.csv'  # Update with your actual file path\n",
        "output_file = 'E:\\Log_anomaly_detection\\Log_anomaly_detection\\sushanth\\preprocess\\group\\proj\\sequence.csv'\n",
        "\n",
        "df = pd.read_csv(input_file)\n",
        "\n",
        "# Group by Block and aggregate unique EventIds as a set for each group\n",
        "grouped_df = df.groupby('Block')['EventId'].agg(set).reset_index()\n",
        "\n",
        "grouped_df.to_csv(output_file, index=False)\n",
        "\n",
        "print(\"Grouping complete. Check the output file:\", output_file)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#labelling for hdfs project data#"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Merging complete. Check the output file: E:\\Log_anomaly_detection\\Log_anomaly_detection\\sushanth\\labelled_sequnce.csv\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "\n",
        "extracted_info_file = 'E:\\Log_anomaly_detection\\Log_anomaly_detection\\sushanth\\preprocess\\group\\proj\\sequence.csv'  # Update with your actual file path\n",
        "label_info_file = 'E:\\Log_anomaly_detection\\Log_anomaly_detection\\sushanth\\preprocess\\HDFS.anomaly_label.csv'  # Update with your actual file path\n",
        "output_file = 'E:\\Log_anomaly_detection\\Log_anomaly_detection\\sushanth\\labelled_sequnce.csv'\n",
        "\n",
        "# Read the extracted info and label info into pandas DataFrames\n",
        "extracted_df = pd.read_csv(extracted_info_file)\n",
        "label_df = pd.read_csv(label_info_file)\n",
        "\n",
        "merged_df = pd.merge(extracted_df, label_df, how='left', left_on='Block', right_on='BlockId')\n",
        "\n",
        "merged_df = merged_df[['EventId', 'Label']]\n",
        "\n",
        "merged_df.to_csv(output_file, index=False)\n",
        "\n",
        "print(\"Merging complete. Check the output file:\", output_file)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#Grouping and labelling for BGL data#"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Grouped data with labels (without Node column) saved to E:\\Log_anomaly_detection\\Log_anomaly_detection\\sushanth\\preprocess\\group\\proj\\BGL_sequnece.csv\n"
          ]
        }
      ],
      "source": [
        "#bgl SEQUNCE FILE GEENRATION\n",
        "import pandas as pd\n",
        "\n",
        "csv_file_path = 'E:\\Log_anomaly_detection\\Log_anomaly_detection\\sushanth\\preprocess\\parse\\proj_data\\BGL_4700k.log_structured.csv'\n",
        "\n",
        "df = pd.read_csv(csv_file_path)\n",
        "\n",
        "grouped_data = df.groupby('Node').agg({'EventId': lambda x: ', '.join(map(str, x)),\n",
        "                                       'Label': lambda x: 'Normal' if all(label == '-' for label in x) else 'Anomalous'}).reset_index()\n",
        "\n",
        "grouped_data.drop('Node', axis=1, inplace=True)\n",
        "\n",
        "output_csv_path = 'E:\\Log_anomaly_detection\\Log_anomaly_detection\\sushanth\\preprocess\\group\\proj\\BGL_sequnece.csv'\n",
        "grouped_data.to_csv(output_csv_path, index=False)\n",
        "\n",
        "print(f\"Grouped data with labels (without Node column) saved to {output_csv_path}\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#Grouping and labelling for Spirit Dataset#"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
        "#grouping for SPIRIT dset!\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "df = pd.read_csv('E:\\Log_anomaly_detection\\Log_anomaly_detection\\sushanth\\preprocess\\parse\\proj_data\\Spirit.log_structured.csv')\n",
        "\n",
        "df['Timestamp'] = pd.to_datetime(df['Timestamp'], unit='s')\n",
        "\n",
        "interval_size = 113\n",
        "\n",
        "df['WindowIndex'] = (df['Timestamp'].astype('int64') // 10**9 // interval_size).astype('int64')\n",
        "\n",
        "grouped_data = df.groupby('WindowIndex')\n",
        "\n",
        "result_list = []\n",
        "\n",
        "for group_name, group_data in grouped_data:\n",
        "    event_ids = set(group_data['EventId'])\n",
        "    label = 'Normal' if all(group_data['Label'] == '-') else 'Anomalous'\n",
        "    result_list.append({'EventIds': event_ids, 'Label': label})\n",
        "\n",
        "result_df = pd.DataFrame(result_list)\n",
        "\n",
        "result_df.to_csv('E:\\Log_anomaly_detection\\Log_anomaly_detection\\sushanth\\preprocess\\group\\proj\\Spirit_sequence.csv', index=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#Grouping and labelling for Thunderbird Dataset#"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "df = pd.read_csv('E:\\Log_anomaly_detection\\Log_anomaly_detection\\sushanth\\preprocess\\parse\\proj_data\\Thunderbird.log_structured.csv')\n",
        "\n",
        "df['Timestamp'] = pd.to_datetime(df['Timestamp'], unit='s')\n",
        "\n",
        "interval_size = 900\n",
        "\n",
        "df['WindowIndex'] = (df['Timestamp'].astype('int64') // 10**9 // interval_size).astype('int64')\n",
        "\n",
        "grouped_data = df.groupby('WindowIndex')\n",
        "\n",
        "result_list = []\n",
        "\n",
        "for group_name, group_data in grouped_data:\n",
        "    event_ids = set(group_data['EventId'])\n",
        "    label = 'Normal' if all(group_data['Label'] == '-') else 'Anomalous'\n",
        "    result_list.append({'EventIds': event_ids, 'Label': label})\n",
        "\n",
        "result_df = pd.DataFrame(result_list)\n",
        "\n",
        "result_df.to_csv('E:\\Log_anomaly_detection\\Log_anomaly_detection\\sushanth\\preprocess\\group\\proj\\Thunderbird_sequence.csv', index=False)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
